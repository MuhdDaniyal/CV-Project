{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForImageTextToText\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"IAMJB/chexpert-mimic-cxr-impression-baseline\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"IAMJB/chexpert-mimic-cxr-impression-baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below 2 cells are rough work (ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode the generated text (this is a text-to-text model)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Output (Impression or Caption):\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode the generated text (this is a text-to-text model)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Output (Impression or Caption):\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model (chexpert-mimic-cxr-impression-baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the image (make sure you have an X-ray image file)\n",
    "image_path = \"/kaggle/input/iu-xray/WhatsApp Image 2024-12-10 at 03.08.25_2bf725f3.jpg\"  # Replace with the path to your X-ray image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Resize and normalize the image (you can check the model's documentation for specific requirements)\n",
    "image = image.convert(\"RGB\")  # Ensure image is in RGB format\n",
    "image = image.resize((224, 224))  # Example resize to match model input size (check the model's input size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "# Use an appropriate image processor (if available for the model)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"IAMJB/chexpert-mimic-cxr-impression-baseline\")\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(images=image, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Forward pass to get the model output\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode the generated text (this is a text-to-text model)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Output (Impression or Caption):\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generated Output (Impression or Caption): 1. no acute cardiopulmonary process. 2. no evidence of free air beneath the diaphragms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for next image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "image_path = \"/kaggle/input/iu-xray/WhatsApp Image 2024-12-10 at 03.08.25_a9d6bb87.jpg\"  # Replace with the path to your X-ray image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Resize and normalize the image (you can check the model's documentation for specific requirements)\n",
    "image = image.convert(\"RGB\")  # Ensure image is in RGB format\n",
    "image = image.resize((224, 224))  # Example resize to match model input size (check the model's input size)\n",
    "\n",
    "# Use an appropriate image processor (if available for the model)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"IAMJB/chexpert-mimic-cxr-impression-baseline\")\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "# Forward pass to get the model output\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode the generated text (this is a text-to-text model)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Output (Impression or Caption):\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "image_path = \"/kaggle/input/iu-xray/WhatsApp Image 2024-12-10 at 03.08.25_a9d6bb87.jpg\"  # Replace with the path to your X-ray image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Resize and normalize the image (you can check the model's documentation for specific requirements)\n",
    "image = image.convert(\"RGB\")  # Ensure image is in RGB format\n",
    "image = image.resize((224, 224))  # Example resize to match model input size (check the model's input size)\n",
    "\n",
    "# Use an appropriate image processor (if available for the model)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"IAMJB/chexpert-mimic-cxr-impression-baseline\")\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "# Forward pass to get the model output\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    outputs = model.generate(**inputs, max_length=100,num_beams=5,       # Beam search for better quality\n",
    "    temperature=0.7,   # Lower temperature for more deterministic output\n",
    "    top_p=0.9)\n",
    "\n",
    "# Decode the generated text (this is a text-to-text model)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Output (Impression or Caption):\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# doesnt do any changes in output so i will not do any parameter changes below if you want to do then do otherwise i don't think so there is any need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now for next model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForImageTextToText\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"IAMJB/chexpert-impression-baseline\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"IAMJB/chexpert-impression-baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "image_path = \"/kaggle/input/iu-xray/WhatsApp Image 2024-12-10 at 03.08.25_2bf725f3.jpg\"  # Replace with the path to your X-ray image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Resize and normalize the image (you can check the model's documentation for specific requirements)\n",
    "image = image.convert(\"RGB\")  # Ensure image is in RGB format\n",
    "image = image.resize((224, 224))  # Example resize to match model input size (check the model's input size)\n",
    "\n",
    "# Use an appropriate image processor (if available for the model)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"IAMJB/chexpert-impression-baseline\")\n",
    "\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "# Forward pass to get the model output\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode the generated text (this is a text-to-text model)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Output (Impression or Caption):\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "image_path = \"/kaggle/input/iu-xray/WhatsApp Image 2024-12-10 at 03.08.25_a9d6bb87.jpg\"  # Replace with the path to your X-ray image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Resize and normalize the image (you can check the model's documentation for specific requirements)\n",
    "image = image.convert(\"RGB\")  # Ensure image is in RGB format\n",
    "image = image.resize((224, 224))  # Example resize to match model input size (check the model's input size)\n",
    "\n",
    "# Use an appropriate image processor (if available for the model)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"IAMJB/chexpert-impression-baseline\")\n",
    "\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "# Forward pass to get the model output\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode the generated text (this is a text-to-text model)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Output (Impression or Caption):\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now for next model (Findings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForImageTextToText\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"IAMJB/chexpert-mimic-cxr-findings-baseline\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"IAMJB/chexpert-mimic-cxr-findings-baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "image_path = \"/kaggle/input/iu-xray/WhatsApp Image 2024-12-10 at 03.08.25_2bf725f3.jpg\"  # Replace with the path to your X-ray image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Resize and normalize the image (you can check the model's documentation for specific requirements)\n",
    "image = image.convert(\"RGB\")  # Ensure image is in RGB format\n",
    "image = image.resize((224, 224))  # Example resize to match model input size (check the model's input size)\n",
    "\n",
    "# Use an appropriate image processor (if available for the model)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"IAMJB/chexpert-mimic-cxr-findings-baseline\")\n",
    "\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "# Forward pass to get the model output\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode the generated text (this is a text-to-text model)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Output (Impression or Caption):\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "image_path = \"/kaggle/input/iu-xray/WhatsApp Image 2024-12-10 at 03.08.25_a9d6bb87.jpg\"  # Replace with the path to your X-ray image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Resize and normalize the image (you can check the model's documentation for specific requirements)\n",
    "image = image.convert(\"RGB\")  # Ensure image is in RGB format\n",
    "image = image.resize((224, 224))  # Example resize to match model input size (check the model's input size)\n",
    "\n",
    "# Use an appropriate image processor (if available for the model)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"IAMJB/chexpert-mimic-cxr-findings-baseline\")\n",
    "\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "# Forward pass to get the model output\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode the generated text (this is a text-to-text model)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Output (Impression or Caption):\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Details of outputs[1]:\", outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Model (BLIP finetuned for chest xray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"daniyal214/finetuned-blip-chest-xrays\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"daniyal214/finetuned-blip-chest-xrays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "image_path = \"/kaggle/input/iu-xray/WhatsApp Image 2024-12-10 at 03.08.25_2bf725f3.jpg\"  # Replace with the path to your X-ray image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Resize and normalize the image (you can check the model's documentation for specific requirements)\n",
    "image = image.convert(\"RGB\")  # Ensure image is in RGB format\n",
    "image = image.resize((224, 224))  # Example resize to match model input size (check the model's input size)\n",
    "\n",
    "# Use an appropriate image processor (if available for the model)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"daniyal214/finetuned-blip-chest-xrays\")\n",
    "\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "# Forward pass to get the model output\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode the generated text (this is a text-to-text model)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Output (Impression or Caption):\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "image_path = \"/kaggle/input/iu-xray/WhatsApp Image 2024-12-10 at 03.08.25_a9d6bb87.jpg\"  # Replace with the path to your X-ray image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Resize and normalize the image (you can check the model's documentation for specific requirements)\n",
    "image = image.convert(\"RGB\")  # Ensure image is in RGB format\n",
    "image = image.resize((224, 224))  # Example resize to match model input size (check the model's input size)\n",
    "\n",
    "# Use an appropriate image processor (if available for the model)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"daniyal214/finetuned-blip-chest-xrays\")\n",
    "\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "# Forward pass to get the model output\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode the generated text (this is a text-to-text model)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Output (Impression or Caption):\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ye wala tou kaam nhi kr raha pata nhi kia bol raha hai xD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('iu_xray.csv')\n",
    "\n",
    "# Define report templates\n",
    "report_templates = {\n",
    "    'No acute cardiopulmonary process': [\n",
    "        \"There is no evidence of pulmonary edema or pneumothorax.\",\n",
    "        \"The lungs are clear.\",\n",
    "        \"The mediastinal and hilar contours are unremarkable.\"\n",
    "    ],\n",
    "    'Mild bibasilar atelectasis': [\n",
    "        \"There is mild bibasilar atelectasis.\",\n",
    "        \"The mediastinal and hilar contours are unremarkable.\",\n",
    "        \"The cardiomediastinal silhouette is within normal limits.\"\n",
    "    ],\n",
    "    'Pneumonia': [\n",
    "        \"In comparison with the study of ___, there is increased opacity in the right upper lobe, consistent with pneumonia.\",\n",
    "        \"The mediastinal and hilar contours are unremarkable.\",\n",
    "        \"The cardiomediastinal silhouette is within normal limits.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define function to generate report\n",
    "def generate_report(image_path):\n",
    "    labels = df.loc[df['Image Path'] == image_path, 'Labels'].values[0]\n",
    "    label_str = ''\n",
    "    for label in labels:\n",
    "        if label in report_templates:\n",
    "            label_str += report_templates[label][0] + '\\n'\n",
    "        else:\n",
    "            label_str += f\"Unknown label: {label}\\n\"\n",
    "\n",
    "    return label_str\n",
    "\n",
    "# Generate reports\n",
    "for index, row in df.iterrows():\n",
    "    image_path = row['Image Path']\n",
    "    labels = row['Labels']\n",
    "\n",
    "    # Load the image (optional)\n",
    "    # img = Image.open(image_path)\n",
    "\n",
    "    # Generate report\n",
    "    report = generate_report(image_path)\n",
    "\n",
    "    print(f\"==============================\")\n",
    "    print(f\"Image path: {image_path}\")\n",
    "    print(report)\n",
    "\n",
    "# Define function to compare with other study\n",
    "def compare_study(label):\n",
    "    return f\"In comparison with the study of ___, there is increased opacity in the right upper lobe, consistent\n",
    "with {label}.\"\n",
    "\n",
    "# Generate reports with comparisons\n",
    "for index, row in df.iterrows():\n",
    "    image_path = row['Image Path']\n",
    "    labels = row['Labels']\n",
    "\n",
    "    # Load the image (optional)\n",
    "    # img = Image.open(image_path)\n",
    "\n",
    "    # Generate report with comparison\n",
    "    label_str = ''\n",
    "    for label in labels:\n",
    "        if label == 'pneumonia':\n",
    "            label_str += compare_study(label) + '\\n'\n",
    "\n",
    "    report = generate_report(image_path)\n",
    "    print(f\"==============================\")\n",
    "    print(f\"Image path: {image_path}\")\n",
    "    print(report)\n",
    "\n",
    "# Define function to add cardiomediastinal silhouette\n",
    "def add_cardiomediastinal_silhouette():\n",
    "    return \"The cardiomediastinal silhouette is within normal limits.\"\n",
    "\n",
    "# Generate reports with cardiomediastinal silhouette\n",
    "for index, row in df.iterrows():\n",
    "    image_path = row['Image Path']\n",
    "    labels = row['Labels']\n",
    "\n",
    "    # Load the image (optional)\n",
    "    # img = Image.open(image_path)\n",
    "\n",
    "    # Generate report with cardiomediastinal silhouette\n",
    "    report = generate_report(image_path)\n",
    "    print(f\"==============================\")\n",
    "    print(f\"Image path: {image_path}\")\n",
    "    print(report + \"\\n\" + add_cardiomediastinal_silhouette())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
